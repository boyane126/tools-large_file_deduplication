## 大文件去重工具

### 简介
最近要将一批数据写入到mongo数据库中，数据文件是使用`txt`格式存储的，一行为一条数据，数据量大概一个文件5亿条左右，总大小12G左右。
里面存在重复数据，所以在录入之前给数据清洗一遍（去重）；

数据格式(data.txt)：
> phone  uui
> 
> 15171776229	3889321679
> 
> 15367900909	3932862879
> 
> 13871220449	5713507389
>
> 15125056129	2428531749
> 
> 18281760639	5748290519
> 
> 18669222419	5652626109

这是用于给大文件去重的工具。大文件是指超过1G或者超过机器内存容量的文件。
因为是大文件，不能一次去重，所以此处采用分片的方式进行去重。

### 实现思路
1. 将文件分片`1G/片`；切片方法：(将行进行hashcode) % (分片数量) ==> 相同的行就会在同一个文件，各分片间唯一。
2. 遍历所有分片，每个文件调用去重函数。将值写入结果文件(文件+qc后缀)。
3. 删除临时文件。

### 使用方法
```shell
cd bin
go run main.go
```
- 配置文件`./conf.json`
```json
{
  "o_dir": "源文件地址",
  "qc_dir": "去重文件地址",
  "burst_size": "分片大小",
  "out_line_num": "丢弃文件首行长度"
}
```
例子：
```json
{
  "o_dir": "./data.txt",
  "qc_dir": "./data.txt.qc",
  "burst_size": 1073741824,
  "out_line_num": 1
}
```